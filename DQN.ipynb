{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from env import MultiAgentEnv\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "from new_env import New_env\n",
    "import gymnasium as gym\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.under_sampling import NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train length 64\n",
    "\n",
    "X_train = pd.read_csv('/Users/louisedurand-janin/Documents/GitHub/HrFlow_Data_Challenge/data/X_train.csv')\n",
    "X_train['employee embedding'] = X_train['employee embedding'].apply(lambda x: np.array(json.loads(x), dtype=np.float64))\n",
    "X_train['company embedding'] = X_train['company embedding'].apply(lambda x: np.array(json.loads(x), dtype=np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y TRAIN\n",
    "y_train = pd.read_csv('/Users/louisedurand-janin/Documents/GitHub/HrFlow_Data_Challenge/data/y_train.csv', index_col=0)\n",
    "encoding_map = {\n",
    "        \"Assistant\": 0,\n",
    "        \"Executive\": 1,\n",
    "        \"Manager\": 2,\n",
    "        \"Director\": 3,\n",
    "        \n",
    "    }\n",
    "\n",
    "y_train = np.array([encoding_map[category] for category in y_train['position']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length 64 !!\n",
    "\n",
    "# Convert embeddings columns in PyTorch tensors \n",
    "employee_embedding_tensor = torch.tensor(np.vstack(X_train['employee embedding'].values), dtype=torch.float64)\n",
    "company_embedding_tensor = torch.tensor(np.vstack(X_train['company embedding'].values), dtype=torch.float64)\n",
    "\n",
    "# Concatenate both\n",
    "combined_tensor = torch.cat([employee_embedding_tensor, company_embedding_tensor], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length 38 !!\n",
    "#Reduce company embeddings to length 6 \n",
    "\n",
    "embeddings_company = np.vstack(X_train['company embedding'].values)\n",
    "\n",
    "\n",
    "# Création de l'objet PCA pour réduire à 6 dimensions\n",
    "pca = PCA(n_components=6)\n",
    "\n",
    "# Fit et transformation des embeddings\n",
    "reduced_embeddings = pca.fit_transform(embeddings_company)\n",
    "\n",
    "print(reduced_embeddings.shape) \n",
    "# Convert embeddings columns in PyTorch tensors \n",
    "employee_embedding_tensor = torch.tensor(np.vstack(X_train['employee embedding'].values), dtype=torch.float64)\n",
    "company_embedding_tensor = torch.tensor(reduced_embeddings, dtype=torch.float64)\n",
    "\n",
    "# Concatenate both\n",
    "combined_tensor = torch.cat([employee_embedding_tensor, company_embedding_tensor], dim=1)\n",
    "print(\"combined tensor shape\", combined_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST \n",
    "\n",
    "X_test = pd.read_csv('/Users/louisedurand-janin/Documents/GitHub/HrFlow_Data_Challenge/data/X_train.csv')\n",
    "X_test['employee embedding'] = X_test['employee embedding'].apply(lambda x: np.array(json.loads(x), dtype=np.float64))\n",
    "X_test['company embedding'] = X_test['company embedding'].apply(lambda x: np.array(json.loads(x), dtype=np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST length 64\n",
    "\n",
    "# Convertir les colonnes d'embeddings en tensors PyTorch\n",
    "employee_embedding_tensor_test = torch.tensor(np.vstack(X_test['employee embedding'].values), dtype=torch.float64)\n",
    "company_embedding_tensor_test = torch.tensor(np.vstack(X_test['company embedding'].values), dtype=torch.float64)\n",
    "\n",
    "# Concaténer les deux tensors le long de la dimension appropriée (axis=1 pour ajouter des colonnes)\n",
    "combined_tensor_test = torch.cat([employee_embedding_tensor_test, company_embedding_tensor_test], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST length 38 \n",
    "\n",
    "embeddings_company_test = np.vstack(X_test['company embedding'].values)\n",
    "\n",
    "\n",
    "# Fit et transformation des embeddings\n",
    "reduced_embeddings_test = pca.transform(embeddings_company_test)\n",
    "\n",
    "print(reduced_embeddings.shape) \n",
    "# Convert embeddings columns in PyTorch tensors \n",
    "employee_embedding_tensor_test = torch.tensor(np.vstack(X_test['employee embedding'].values), dtype=torch.float64)\n",
    "company_embedding_tensor_test = torch.tensor(reduced_embeddings_test, dtype=torch.float64)\n",
    "\n",
    "# Concatenate both\n",
    "combined_tensor_test = torch.cat([employee_embedding_tensor_test, company_embedding_tensor_test], dim=1)\n",
    "print(\"combined tensor shape\", combined_tensor_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y TEST\n",
    "y_test = pd.read_csv(\"/Users/louisedurand-janin/Documents/GitHub/HrFlow_Data_Challenge/data/y_test.csv\", index_col=1)\n",
    "encoding_map = {\n",
    "        \"Assistant\": 0,\n",
    "        \"Executive\": 1,\n",
    "        \"Manager\": 2,\n",
    "        \"Director\": 3,\n",
    "    }\n",
    "y_test = np.array([encoding_map[category] for category in y_test['position']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=0)\n",
    "X_oversampled, y_oversampled = ros.fit_resample(combined_tensor, y_train)\n",
    "X_oversampled, y_oversampled = shuffle(X_oversampled, y_oversampled, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "undersample = NearMiss(version=1, n_neighbors=3)\n",
    "X_undersampled, y_undersampled = undersample.fit_resample(np.array(combined_tensor), y_train)\n",
    "X_undersampled, y_undersampled = shuffle(X_undersampled, y_undersampled, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device):\n",
    "        self.capacity = capacity # capacity of the buffer\n",
    "        self.data = []\n",
    "        self.index = 0 # index of the next cell to be filled\n",
    "        self.device = device\n",
    "    def append(self, s, a, r, s_, d):\n",
    "        if len(self.data) < self.capacity:\n",
    "            self.data.append(None)\n",
    "        self.data[self.index] = (s, a, r, s_, d)\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.data, batch_size)\n",
    "        return list(map(lambda x:torch.Tensor(np.array(x)).to(self.device), list(zip(*batch))))\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_action(network, state):\n",
    "    device = \"cuda\" if next(network.parameters()).is_cuda else \"cpu\"\n",
    "    with torch.no_grad():\n",
    "        Q = network(torch.Tensor(state).unsqueeze(0).to(device))\n",
    "        #print(\"Q\", Q)\n",
    "        return torch.argmax(Q).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class dqn_agent:\n",
    "    def __init__(self, config, model, memory):\n",
    "        device = \"cuda\" if next(model.parameters()).is_cuda else \"cpu\"\n",
    "        self.gamma = config['gamma']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.nb_actions = config['nb_actions']\n",
    "        self.memory = memory\n",
    "        self.epsilon_max = config['epsilon_max']\n",
    "        self.epsilon_min = config['epsilon_min']\n",
    "        self.epsilon_stop = config['epsilon_decay_period']\n",
    "        self.epsilon_delay = config['epsilon_delay_decay']\n",
    "        self.epsilon_step = (self.epsilon_max-self.epsilon_min)/self.epsilon_stop\n",
    "        self.model = model.double()\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    def gradient_step(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            X, A, R, Y, D = self.memory.sample(self.batch_size)\n",
    "            Y = Y.double()\n",
    "            A = A.double()\n",
    "            X = X.double()\n",
    "            #print(X.shape)\n",
    "            #print(Y.shape)\n",
    "            #print(A.shape)\n",
    "            QYmax = self.model(Y).max(1)[0].detach()\n",
    "\n",
    "            #print(QYmax.shape)\n",
    "            #update = torch.addcmul(R, self.gamma, 1-D, QYmax)\n",
    "            update = torch.addcmul(R, 1-D, QYmax, value=self.gamma)\n",
    "            QXA = self.model(X).gather(1, A.to(torch.long))\n",
    "            loss = self.criterion(QXA, update.unsqueeze(1))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step() \n",
    "    def train(self, env, max_episode):\n",
    "        episode_return = []\n",
    "        episode = 0\n",
    "        episode_cum_reward = 0\n",
    "        state, _ = env.reset(True)\n",
    "        epsilon = self.epsilon_max\n",
    "        step = 0\n",
    "        donebis=False\n",
    "        greed_action_stay=0\n",
    "        while episode < max_episode:\n",
    "            # update epsilon\n",
    "            if step > self.epsilon_delay:\n",
    "                epsilon = max(self.epsilon_min, epsilon-self.epsilon_step)\n",
    "\n",
    "            # select epsilon-greedy action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = greedy_action(self.model, state)\n",
    "                greed_action_stay+=1\n",
    "                if greed_action_stay==2:\n",
    "                    donebis=True\n",
    "\n",
    "            # step\n",
    "            predicted_next_position, predicted_next_emb_state, reward, done, _ = env.step(action)\n",
    "            #print(\"action\", action)\n",
    "            #print(\"next state\", predicted_next_position)\n",
    "            #print(\"reward\", reward)\n",
    "            #print(\"target\",y_undersampled[env.index])\n",
    "            #self.memory.append(state, action, reward, predicted_next_emb_state, done)\n",
    "            episode_cum_reward += reward\n",
    "\n",
    "            # train\n",
    "            self.gradient_step()\n",
    "\n",
    "            # next transition\n",
    "            step += 1\n",
    "            if done or donebis:\n",
    "                greed_action_stay = 0\n",
    "                donebis=False\n",
    "                \n",
    "                episode += 1\n",
    "                print(\"Episode \", '{:3d}'.format(episode), \n",
    "                      \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                      \", batch size \", '{:5d}'.format(len(self.memory)), \n",
    "                      \", episode return \", '{:4.1f}'.format(episode_cum_reward),\n",
    "                      sep='')\n",
    "                if episode>=env.Emb.shape[0]:\n",
    "                    state,_=env.reset(True)\n",
    "                else:\n",
    "                    state, _ = env.reset(False)\n",
    "                episode_return.append(episode_cum_reward)\n",
    "                episode_cum_reward = 0\n",
    "            else:\n",
    "                state = predicted_next_emb_state\n",
    "\n",
    "        return episode_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "#cartpole = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Declare network\n",
    "\n",
    "state_dim = 39\n",
    "n_action = 2\n",
    "nb_neurons=64\n",
    "DQN = torch.nn.Sequential(nn.Linear(state_dim, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(nb_neurons, n_action)).to(device)\n",
    "\n",
    "# DQN config\n",
    "config = {'nb_actions': 2,\n",
    "          'learning_rate': 0.001,\n",
    "          'gamma': 0.95,\n",
    "          'buffer_size': 10000,\n",
    "          'epsilon_min': 0.01,\n",
    "          'epsilon_max': 1,\n",
    "          'epsilon_decay_period': 1000,\n",
    "          'epsilon_delay_decay': 20,\n",
    "          'batch_size': 20}\n",
    "\n",
    "# Train agent\n",
    "agent = dqn_agent(config, DQN.double(), replay_buffer)\n",
    "env = New_env(torch.tensor(X_undersampled, dtype=torch.float64), torch.tensor(y_undersampled, dtype=torch.float64))\n",
    "scores = agent.train(env, 50000)\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on TRAIN\n",
    "career_env_test = New_env(torch.tensor(X_undersampled, dtype=torch.float64), torch.tensor(y_undersampled, dtype=torch.float64))\n",
    "pred=[]\n",
    "y_pred = []\n",
    "s,_ =  career_env_test.reset(True)\n",
    "i=0\n",
    "#for t in tqdm(range(len(y_test))):\n",
    "for t in tqdm(range(len(y_undersampled))):\n",
    "    for k in range(5):\n",
    "        a = greedy_action(agent.model,s)\n",
    "\n",
    "        predicted_next_position, predicted_next_emb_state, reward, d, _ = career_env_test.step(a)\n",
    "\n",
    "        s = predicted_next_emb_state\n",
    "        if t in [0,1,2,3]:\n",
    "            print(\"action chosen\", a)\n",
    "\n",
    "            print(\"s\", predicted_next_position)\n",
    "            print(\"target\", y_undersampled[i])\n",
    "        #if a ==0:\n",
    "         #   break\n",
    "        \n",
    "        if d:\n",
    "            break\n",
    "    i+=1\n",
    "    pred.append(s)\n",
    "    y_pred.append(predicted_next_position.item())\n",
    "    s,_=career_env_test.reset(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y_pred, y_undersampled, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "career_env_test = New_env(combined_tensor_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=[]\n",
    "y_pred = []\n",
    "s,_ =  career_env_test.reset(True)\n",
    "i=0\n",
    "for t in tqdm(range(len(y_test))):\n",
    "#for t in tqdm(range(4)):\n",
    "    for k in range(4):\n",
    "        a = greedy_action(agent.model,s)\n",
    "\n",
    "        predicted_next_position, predicted_next_emb_state, reward, d, _ = career_env_test.step(a)\n",
    "\n",
    "        s = predicted_next_emb_state\n",
    "        if t in [0,1,2,3]:\n",
    "            print(\"action chosen\", a)\n",
    "\n",
    "            print(\"s\", predicted_next_position)\n",
    "            print(\"target\", y_test[i])\n",
    "        #if a ==0:\n",
    "         #   break\n",
    "        \n",
    "        if d:\n",
    "            break\n",
    "    i+=1\n",
    "    pred.append(s)\n",
    "    y_pred.append(predicted_next_position.item())\n",
    "    s,_=career_env_test.reset(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y_pred, y_test, average=\"macro\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
